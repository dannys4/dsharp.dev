<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[research_notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://dsharp.dev/</link><image><url>https://dsharp.dev/lib/media/favicon.png</url><title>research_notes</title><link>https://dsharp.dev/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Fri, 24 May 2024 21:38:01 GMT</lastBuildDate><atom:link href="https://dsharp.dev/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Fri, 24 May 2024 21:37:57 GMT</pubDate><copyright><![CDATA[D Sharp]]></copyright><ttl>60</ttl><dc:creator>D Sharp</dc:creator><item><title><![CDATA[Polynomial Recurrence Flow]]></title><description><![CDATA[ 
 <br><br>I'm particularly interested if this can be extended to work for flows of functions and, in particular, orthogonal polynomials. Recall that<br>
<br><br>It's fairly clear that, if you take the inner product of both sides with , we get that<br><br>Similarly,<br>
We can simplify the numerator for  as  due to wikipedia. The second form of the norm does not seem very helpful.<br>Assuming this is correct, we further constrict our polynomials to have unit magnitude, which automatically indicates that  and   by construction. Now suppose we construct  as the th orthonormal polynomial w.r.t. measure . We should get that  is a function representing the th recurrence term at time .<br>One could show that (assuming unit variance of the polynomials)<br><br>I have no idea what  should be due to the representation of . Additionally, note that  and  depend on  and . At this point, this is a disgusting equation whic h no one would ever want to solve. At least I don't want to.<br>Another addition is that this becomes somewhat intractable in higher than one dimension due to the finicky nature of recurrence relations on multivariate polynomials.<br>Finally, I'm not entirely sure what one would linearize here, or how this could possibly be made more efficient.<br><br>The subsequent question is what else you would transport. One idea I like is transporting roots so that, at the end of the transport, you approximate the exact roots of the orthonormal polynomials (then one could maybe approximate the value of the Christoffel function or something? I'm not sure)<br>Alternatively, another idea would be somehow transporting the weights so that you keep the points and the weights change according to the transport (in the vein of tempered importance weighting?)<br><br>Generally, we know , where  and . Because I'm lazy, I'm going to drop the hat (even though technically  doesn't agree with the above definition of ). Then, we see the more complicated time derivative of  as<br><br>Via our abuse of notation, however,  stays the same as <a data-href="#^c0ec95" href="https://dsharp.dev/#^c0ec95" class="internal-link" target="_self" rel="noopener">^c0ec95</a>. On the other hand, we get<br>
<br><br>Suppose we know  up to a constant. Then, set . Then, , so we observe<br><br>and<br><br>Additionally, on the corner cases, assuming  we know<br><br>Now we can formulate an algorithm:<br><br>Assume: , <br>
Input: (possibly unnormalized?) density ,  for , step size <br>
Output:  for , which integrate , respectively<br>
<br>for <br>
- Calculate  from .<br>
- Set   and renormalize <br>
- Note that , so calculate <br>
- for ,<br>
- for each quadrature point <br>
- Calculate <br>
- Calculate <br>
- Calculate <br>
- Estimate $$<br>
\begin{align}<br>
\dot{a}{k}(t) &amp;\approx \sum\limits{i=1}^{N}w{t}^{(i)}x^{(i)}{t}p{k}^{(i)}\left[p{k}^{(i)}v^{(i)} + 2\dot{p}{k}^{(i)}\right]\<br>
\dot{\ell}{k}(t) &amp;\approx \sum\limits{i=1}^{N}w{t}^{(i)}p{k}^{(i)}\left[p{k}^{(i)}v^{(i)} + 2\dot{p}_{k}^{(i)}\right]<br>
\end{align}


<br>Calculate  from .
<br>Note that this approximates the Euler discretization of the ODE of . However, <a data-tooltip-position="top" aria-label="^diff-a-l" data-href="#^diff-a-l" href="https://dsharp.dev/#^diff-a-l" class="internal-link" target="_self" rel="noopener">the inner for loop</a> approximates the derivative of , given the values of , so one could use any other scheme (e.g., Runge-Kutta).]]></description><link>https://dsharp.dev/thesis/research/quadrature/polynomial-recurrence-flow.html</link><guid isPermaLink="false">Thesis/Research/Quadrature/Polynomial Recurrence Flow.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 21:34:05 GMT</pubDate></item><item><title><![CDATA[On integration in a measure transport framework]]></title><description><![CDATA[ 
 <br><br>Setting the stage<br><br>We are interested in, for , approximating the expectation  by a quadrature rule of the form . There are a few central questions:<br>
<br>What can we prove about  when integrating ?
<br>What can we prove about  when integrating ?
<br>How does the error between  and  affect the difference between  and ?
<br>How do we consider all of these to bound the error on 
<br>How do we choose  to minimize  in some appropriate norm? Does this change how we choose the loss?
<br><br><br>Quadrature generally, to some degree, approximates inner products via polynomials of some kind. Gauss quadrature gets phenomenal convergence via the fact that  is exact on polynomials up to degree . Clenshaw-Curtis is exact on polynomials of degree , and many other quadrature rules share this property. Needless to say, this hinges precisely on the fact that  , when smooth, can be approximated very well by polynomials; further, if  "largely made of" low-degree polynomials (i.e. any polynomial approximation, e.g. PCE, has rapidly decaying coefficients), we know that this quadrature rule works very well. Suppose we have , orthogonal under ; let  be the degree  PCE of . We know that, for sufficiently smooth ,<br><br>which comes from Parseval's identity (see Sagiv 22). What we want is that, given a function  and a set , we can create an approximation<br><br>or something like that. Note here that  is a complete degree of freedom-- we can choose it however we wish. Note here a few things:<br>
<br> is probably not a polynomial, and  is almost certainly not a polynomial
<br>Because of this,  is almost definitely not a polynomial
<br>Regardless, assuming orthonormality of , we get that

Similar to this, we get

Therefore, a spectral projection of  onto  is equivalent to a spectral projection of  onto , and thus we inherit any convergence properties of spectral approximation on  of the function .
<br><br>Suppose we map from the reference  to  via the inverse CDF of the Normal distribution as . Then,  are naturally the Legendre polynomials and  is a Legendre polynomial composed with the CDF of  which, while smooth, is entirely bounded for any given . Therefore, a -point quadrature rule will only capture behavior on the domain for , which grows rather slowly (i.e., the effective domain of this quadrature rule is rather small). However, the basis functions  are fairly smooth so we can get convergence faster than the  rate cited above.<br><br>In the above example, the problem is ostensibly that the transport  maps a distribution with "lighter tails" (in fact, tails that are exactly zero) to a distribution with "heavier tails". It then stands to reason that we might do better if we do the opposite way. Consider now  (the reference is Gaussian now) and , i.e., . We are initially pleased that then  are orthogonal polynomials! However, there are a few problems that crop up.<br>
<br>; so, if we use a quadrature rule with  odd, the middle point index  (which receives the most weight!) will stay at zero, where  Therefore, our "practitioner's method" places a lot of weight where the density places zero weight.
<br>The polynomials  are indeed orthogonal and clearly span ; however, they aren't a "traditional family" since , so the th term is a degree  polynomial. Therefore, if , we will never be able to integrate exactly!
<br>Indeed, the rate of convergence for  is abyssmal. The rate is precisely the rate of decay of the coefficients when projecting  onto  (consider Parseval).
<br>What I take away from these examples is that having "lighter" tails isn't enough; you want the same tails, i.e., the transport  should be exactly linear in the tails. In this sense, we might consider regularizing by the log of the Sobolev norm or something.<br><br>Suppose  is a RKHS with kernel . It would be nice to get something of the following form:<br><br>where  is the discrete distribution using the pushforward of  on a quadrature w.r.t. .<br><br>This "moves the onus onto ", so-to-speak. For a given kernel,  will smooth out the "high moments" of the distributions and only care about the broad strokes of similarity. Then, as long as  broadly captures the features of , we get that the bias of the quadrature is largely dependent on how bad the function is itself. In some sense, this is the "easier" problem to solve, because most convergence of quadrature rules depend on how large the norm of  is in some sobolev sense.<br><br>
<br>Use "heat" to diffuse  into , and then perform transport on  for small (or decreasing) .
<br>If  is our integral and  is our -point approximation of the integral , do something like


<br>Note that if , we have that . Similar if in  (sobolev).
<br>Sagiv, Baptista paper is a very loose bound by strong engine for bounding problems.
<br>Suppose we know Gaussian quadrature points for  and consider a map  that draws a voronoi tesselation on the domain and maps deterministically to these quadrature points. This is an atrocious sampling map, but has wonderful expectation approximation properties if it maps gaussian points to gaussian points (for a specific construction).
<br>Consider distributions with compact support?
<br>Sobolev norm is introduced in statistics to ensure uniqueness of the map 
<br>What is the geometry induced by the MMD? Even less formally, more pedestrian, how do simple functions (e.g. piecewise linear) functions morph under ?

<br>In this sense, can we say anything about, e.g., the trapezoidal rule?


<br>Instead of choosing  to minimize the error, maybe just minimize an upper bound on the error.

<br>Something to do with the Sagiv paper bound.
<br>How to tune ? I personally want to use sobolev regularization, but Amir is a little skeptical


<br>His process:
<br>
<br>Create approximation theory for transformed space

<br>From parseval and spectral approximation, etc (This is where I think sobolev norm kicks in!!)


<br>Look at the error of the expectations of  between target and pushforward to get interesting bound
<br>Use bounds on error of pushforward to encourage construction in the right place
]]></description><link>https://dsharp.dev/thesis/research/quadrature/practitioner's-quadrature.html</link><guid isPermaLink="false">Thesis/Research/Quadrature/Practitioner's Quadrature.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 21:28:57 GMT</pubDate></item><item><title><![CDATA[Wasserstein-Fisher-Rao Gradient Flow]]></title><description><![CDATA[ 
 <br><br>Suppose we have two probability measures, , and we would like to find the distance between them. While one naturally might consider the Wasserstein metric, which (by Brenier-Benamou) is given by<br><br>this is well-studied and has well-known restrictions. An alternative here would be the Fisher-Rao distance,<br><br>Note that this is distance is constrained by a reaction PDE instead of a continuity equation. We can see that<br><br>where . Since we know , then the total mass can't possibly change (consider discretizing this via Euler and noting that it won't change for any step-size , so it converges as ). One can see that the (adjusted) Fisher-Rao induces a geodesic of the form<br><br>but this is irrelevant to the topic at hand.<br><br>The PDE constraining the Wasserstein-Fisher-Rao metric will intuitively be<br><br>i.e., we use velocity field  and "reaction function"  to govern the dynamics of . For a given functional , we would like to find a "Wasserstein-Fisher-Rao" gradient flow. In particular, we would like to find a velocity field  and reaction function  which determine a  s.t.  minimizes  using constant-speed geodesics in the WFR geometry. To do this, we first must define a "Wasserstein-Fisher-Rao gradient". Generally one defines such gradients by writing  as  for some  governing the PDE (TODO: elaborate?). Observe that<br><br>Then, if I have a measure , I would end up with something like<br><br>See, e.g., <a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2301.01766" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2301.01766" target="_blank">YWR23</a><br><br>At first, we may be interested in the KL-divergence function often used in gradient flows, which (for a given distribution ) has pertinent information<br><br>but if we keep  as a discrete measure, we will not be able to ever estimate . It is possible to use a kernelized version of  in the vein of SVGD, but I'm not sold. Alternatively, I could use score matching. For the record, this gives<br><br>I don't think I need to center the weight derivative? Anyway, if  is truly a discrete distribution, then this is undefined. If you kernelize it.... I'm not entirely sure, but it'd give something I guess? On the other hand, perhaps you could match the score (or score ratio). See <a data-href="Polynomial Score Matching" href="https://dsharp.dev/thesis/research/transport-intrinsics/polynomial-score-matching.html" class="internal-link" target="_self" rel="noopener">Polynomial Score Matching</a>, which would need to be modified for accounting for , which weights the true .<br><br>Suppose we now have<br><br>where  is the norm in the RKHS induced by the kernel . <a data-tooltip-position="top" aria-label="https://akorba.github.io/resources/swslstuttgart2019slides.pdf" rel="noopener" class="external-link" href="https://akorba.github.io/resources/swslstuttgart2019slides.pdf" target="_blank">One can find</a> that<br><br>where  refers to the gradient in the second argument. Suppose that . Then,<br>
. More generally, if  for  positive and  symmetric distance, then you get  which is generally pretty easy to compute. This gives the update equation as<br><br><br>What's nice here is that this is "forgetful" of our initial statue , assuming the convexity and all that (probably not the case, but still). The choice of  is one of preconditioning, it's not an end-all-be-all choice in theory. So far in our transport quadrature, our choice of reference basically entirely constrains the points to be the pushforward of the reference quadrature under the map with no change to the weights, where this isn't necessarily the case.<br><br>Take our Hilbert space  to be polynomials up to degree  with inner product<br><br>so our kernel becomes the Christoffel-Darboux kernel of<br><br>with  the  norm. I.e., this kernel projects onto the polynomials , which are orthogonal w.r.t. . Without loss of generality, assume . Obviously, for any function , we see<br><br>which means the embedding of an arbitrary function  into our RKHS is exactly the orthogonal projection of . Our maximum mean discrepancy becomes<br><br>Assume we want to match points and weights , i.e., discrete measure  to some target . We see<br><br>^3be7e2<br>
with . Then, we get that<br><br>and somewhat less obviously, suppose we abuse notation to consider a diagonal matrix  with diagonal entry  equal to ; matrix calculus dictates<br><br>which indicates the trace is operating on a matrix with only one nonzero column ; therefore,  is the th entry in the th column for  and zero otherwise. This corresponds to<br><br>Here we maintain control of three things: . We know that  induces the orthogonal polynomial family , and this gets normalized (with respect to ), so we don't get to choose which scaling for the family. Notably, for fixed , this becomes a quadratic loss in , so we can use a linear solve to find the solution. For the case of finding  as well, it seems straightforward enough to attempt to just choose  and optimize over these values. Unfortunately, even in one dimension, the Gaussian quadrature being unique does not mean that it will be easy to find in this setting. In higher dimensions, this will be very difficult without something of a greedy procedure.<br><br>We now assume data from  and want to estimate a quadrature via discrete  as  and we have reference distribution . Taking  as our "preconditioner", we estimate  for  from data (perhaps), then we have (for  quadrature points)<br><br>One can compare and contrast this with the minimization problem given in <a data-href="#^021c59" href="https://dsharp.dev/#^021c59" class="internal-link" target="_self" rel="noopener">^021c59</a>. While they are very similar, note that the geometry of the static problem by  on the points, and the WFR places the scaling of  on the weights. This is a slight change, but can make remarkable differences due to imposing different geometries.]]></description><link>https://dsharp.dev/thesis/research/quadrature/wasserstein-fisher-rao-gradient-flow.html</link><guid isPermaLink="false">Thesis/Research/Quadrature/Wasserstein-Fisher-Rao Gradient Flow.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 21:36:19 GMT</pubDate></item><item><title><![CDATA[Polynomial Score Matching]]></title><description><![CDATA[ 
 <br>Suppose we have data  and you want to estimate  from the data. One way to do this is via score matching, i.e., estimate a function representing the gradient of the log PDF  (obligatory note that "score" in statistics usually refers to something of the form , which is not the same). Unless otherwise specified, consider  to indicate the gradient of a function in  and  to be the <a data-tooltip-position="top" aria-label="https://www.wikiwand.com/en/Laplace_operator" rel="noopener" class="external-link" href="https://www.wikiwand.com/en/Laplace_operator" target="_blank">Laplacian</a>. To formulate this,  we use the Fisher divergence as in <a data-tooltip-position="top" aria-label="https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf" rel="noopener" class="external-link" href="https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf" target="_blank">Hyvaerinen 05</a> and formulate it as a loss function of parameters  of some , an approximation to the PDF .<br><br>where the last step comes from integration by parts assuming  are zero at the boundaries of the domain (i.e., ). We let  as a slightly "nicer" loss function, which has the same minimizer  as . Notably, this loss function doesn't require that  is even a proper density, it just requires the ability to integrate with respect to  (e.g., from samples), and evaluate derivatives of , which is what we try to model.<br> It should be observed that, often in score-matching literature, researchers will parameterize the gradient of , not the function itself; this can produce a nonphysical solution since the learned function won't necessarily even be of the form  for any particular . Now, we parameterize  where  are scalar-valued functions (assume multivariate polynomial terms); without loss of generality, assume that  as , the boundary of our domain. Then, we know that<br><br>Therefore, we get that<br><br>If we then set  and , then our loss ends up becoming quadratic in  as , which is known to have minimizer . It's particularly interesting that , i.e., it is square and finite dimensional, even though our expectation requires an "infinite" number of evaluations. Contrast this with a Vandermonde-like matrix, which will grow with the number of samples. The next interesting point is to consider the elements of  and .<br><br><br>Now suppose  and  for some multi-index set , i.e., we want to find estimate the coefficients  for  to approximate . We know that  and that  since all probabilist Hermite polynomials are orthogonal to , so<br><br>for some , where  is the elementary vector with all zeros except for  in entry  and  by convention for . Similarly,<br><br>implying that  is precisely diagonal. Then, since we have , we get that  when  for some . Now suppose  is of this form. We must have that<br><br>which implies that   (notably we must have that there is some constant that normalizes this which we can't find via score-matching). This is precisely giving that , i.e.,  will converge exactly.<br><br>Suppose we have  samples  and the input dimension is  as before, and consider . We can formulate that<br><br>where  is a function such that . Then, we estimate  as<br><br>so in this case, we can generally assume  (though this is technically only an upper bound, linear independence of the functions should give this handily). Contrast this to the Vandermonde case, where , which gives a matrix  of rank . While these two numbers seem different, there is actually some parity to them; score matching attempts to match , which gives the illusion of an extra  dimension of freedom from the gradient. Therefore, to adjust for this extra freedom, the number of parameters should be scaled by the reciprocal of the dimension when comparing against .<br>We can also form , where  is applied elementwise. Then, we wish to solve . Obviously, this can be done uniquely when . However, in the "overparameterized" case, there are clearly an infinite number of solutions by simple linear algebra. A traditional technique would be to solve<br><br>i.e., find the minimum norm solution. This can be done via allowing , where  and  give the orthogonal eigenvectors and their nonzero eigenvalues, respectively (we know that  is symmetric positive semi-definite since it is the sum of SPSD matrices). Then we know that  will be the optimal solution  (Trefethen and Bau?).<br>However, while this has an optimal solution in closed form, the objective  is not at all well motivated in this context, besides that it will ensure  doesn't have entries diverging to infinity. Perhaps a better solution would be to minimize the Kullback-Leibler divergence, . By definition,<br><br>While I think this is a convex function (the rightmost term is reminiscient of a log-sum-exp), it's not really tractable as an objective even when approximating the expectation with an average over the empirical distribution of . The integral over  is not tractable in closed form. However, examining the role of this term suggests some regularization against higher frequency . This intuition comes from that, if  is highly oscillatory and  is nonzero,  will often be true and thus we will be far from the minimum. In this sense, we approximate the KL as<br><br>For simplicity, imagine  as a diagonal matrix with  scaling with  in some way. While  will clearly have a minimizer at , this is physically meaningless due to the relaxation of the log-sum-exp term, i.e., this will not be guaranteed to minimize any statistical divergence in any manner. However, it seems reasonable to return to the Fisher divergence score-matching setting to say<br><br><br>Generally, the neural networks for diffusion models are minimizing functions of the form<br><br>which approximates the loss<br><br>where  is some density that we prescribe to the "importance" of different time-points and  is the measure of  following the SDE<br><br>with . Suppose now that we consider  with . Then,<br><br>where we abuse notation to say  and  . Finally, suppose we separate the random variables from time in our functions via  Then,<br><br>However, consider that we know . Then, we should have .]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/polynomial-score-matching.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Polynomial Score Matching.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Tue, 21 May 2024 12:52:13 GMT</pubDate></item><item><title><![CDATA[Transport Intrinsics]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="Map jacobian determinant" href="https://dsharp.dev/thesis/research/transport-intrinsics/map-jacobian-determinant.html" class="internal-link" target="_self" rel="noopener">Map jacobian determinant</a>
<br><a data-href="Max Map Adaptation" href="https://dsharp.dev/thesis/research/transport-intrinsics/max-map-adaptation.html" class="internal-link" target="_self" rel="noopener">Max Map Adaptation</a>
<br><a data-href="Optimizing Linear Maps (new draft)" href="https://dsharp.dev/thesis/research/transport-intrinsics/optimizing-linear-maps-(new-draft).html" class="internal-link" target="_self" rel="noopener">Optimizing Linear Maps (new draft)</a>
<br><a data-href="Polynomial Score Matching" href="https://dsharp.dev/thesis/research/transport-intrinsics/polynomial-score-matching.html" class="internal-link" target="_self" rel="noopener">Polynomial Score Matching</a>
<br><a data-href="Rectified Linear Parameterization" href="https://dsharp.dev/thesis/research/transport-intrinsics/rectified-linear-parameterization.html" class="internal-link" target="_self" rel="noopener">Rectified Linear Parameterization</a>
<br><a data-href="Sigmoid clustering" href="https://dsharp.dev/thesis/research/transport-intrinsics/sigmoid-clustering.html" class="internal-link" target="_self" rel="noopener">Sigmoid clustering</a>
<br>]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/transport-intrinsics.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Transport Intrinsics.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 20:44:20 GMT</pubDate></item><item><title><![CDATA[Thesis Progress]]></title><description><![CDATA[ 
 <br><br><br><a data-tooltip-position="top" aria-label="https://github.com/dannys4/MultivariateExpansions.jl" rel="noopener" class="external-link" href="https://github.com/dannys4/MultivariateExpansions.jl" target="_blank">MultivariateExpansions.jl</a><br><a data-tooltip-position="top" aria-label="https://github.com/dannys4/MultiIndexing.jl" rel="noopener" class="external-link" href="https://github.com/dannys4/MultiIndexing.jl" target="_blank">MultiIndexing.jl</a><br><a data-tooltip-position="top" aria-label="https://github.com/dannys4/EnsembleFiltering.jl/" rel="noopener" class="external-link" href="https://github.com/dannys4/EnsembleFiltering.jl/" target="_blank">EnsembleFiltering.jl</a><br><a data-tooltip-position="top" aria-label="https://github.com/MeasureTransport/MParT" rel="noopener" class="external-link" href="https://github.com/MeasureTransport/MParT" target="_blank">MParT</a><br>Input: multi-index set mset_full, which is assumed downward-closed
Output: quad_rules: set of tensor-product quadratures and their (possibly negative) associated cardinality `Vector{Tuple{Midx, Int}}`
keep_looping = true
quad_rules = []

while keep_looping
	# Form the mset for this loop
	look at indexes of occurrences that are not 1.
	form them into mset_loop
	get reduced frontier frontier of mset_loop
	
	# Adjust each frontier member and reindex their ancestors' number of occurrences
	for idx_midx_loop in frontier:
		# Access the adjustment for this frontier member
		get index idx_midx_full corresponding to the multi-index idx_midx_loop in mset_full
		set j = occurrences[idx_midx_full] - 1, e.g., if idx_midx_full occurs $j+1=-1$ times, then $j=-2$.
		occurrences[idx_midx_full] = 1

		# Reindex the number of occurrences of the backward ancestors
		get backward ancestors of idx_midx_loop
		for each ancestor idx_back_loop of idx_midx_loop
			get original index idx_back_full of idx_back_loop (i.e. index of idx_back_loop w.r.t. mset_full, not mset_loop)
			occurrences[idx] -= j
		end for
		push (idx_midx_full, j) onto quad_rules
	end for
	keep_looping = occurrences is not identically 1
end while

return quad_rules
CopySmolyak<br>Suppose we have data  and you want to estimate  from the data. One way to do this is via score matching, i.e., estimate a function representing the gradient of the log PDF  (obligatory note that "score" in statistics usually refers to something of the form , which is not the same). Unless otherwise specified, consider  to indicate the gradient of a function in  and  to be the <a data-tooltip-position="top" aria-label="https://www.wikiwand.com/en/Laplace_operator" rel="noopener" class="external-link" href="https://www.wikiwand.com/en/Laplace_operator" target="_blank">Laplacian</a>. To formulate this,  we use the Fisher divergence as in <a data-tooltip-position="top" aria-label="https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf" rel="noopener" class="external-link" href="https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf" target="_blank">Hyvaerinen 05</a> and formulate it as a loss function of parameters  of some , an approximation to the PDF .<br><br>where the last step comes from integration by parts assuming  are zero at the boundaries of the domain (i.e., ). We let  as a slightly "nicer" loss function, which has the same minimizer  as . Notably, this loss function doesn't require that  is even a proper density, it just requires the ability to integrate with respect to  (e.g., from samples), and evaluate derivatives of , which is what we try to model.<br> It should be observed that, often in score-matching literature, researchers will parameterize the gradient of , not the function itself; this can produce a nonphysical solution since the learned function won't necessarily even be of the form  for any particular . Now, we parameterize  where  are scalar-valued functions (assume multivariate polynomial terms); without loss of generality, assume that  as , the boundary of our domain. Then, we know that<br><br>Therefore, we get that<br><br>If we then set  and , then our loss ends up becoming quadratic in  as , which is known to have minimizer . It's particularly interesting that , i.e., it is square and finite dimensional, even though our expectation requires an "infinite" number of evaluations. Contrast this with a Vandermonde-like matrix, which will grow with the number of samples. The next interesting point is to consider the elements of  and .<br><br><br>Now suppose  and  for some multi-index set , i.e., we want to find estimate the coefficients  for  to approximate . We know that  and that  since all probabilist Hermite polynomials are orthogonal to , so<br><br>for some , where  is the elementary vector with all zeros except for  in entry  and  by convention for . Similarly,<br><br>implying that  is precisely diagonal. Then, since we have , we get that  when  for some . Now suppose  is of this form. We must have that<br><br>which implies that   (notably we must have that there is some constant that normalizes this which we can't find via score-matching). This is precisely giving that , i.e.,  will converge exactly.<br><br>Suppose we have  samples  and the input dimension is  as before, and consider . We can formulate that<br><br>where  is a function such that . Then, we estimate  as<br><br>so in this case, we can generally assume  (though this is technically only an upper bound, linear independence of the functions should give this handily). Contrast this to the Vandermonde case, where , which gives a matrix  of rank . While these two numbers seem different, there is actually some parity to them; score matching attempts to match , which gives the illusion of an extra  dimension of freedom from the gradient. Therefore, to adjust for this extra freedom, the number of parameters should be scaled by the reciprocal of the dimension when comparing against .<br>We can also form , where  is applied elementwise. Then, we wish to solve . Obviously, this can be done uniquely when . However, in the "overparameterized" case, there are clearly an infinite number of solutions by simple linear algebra. A traditional technique would be to solve<br><br>i.e., find the minimum norm solution. This can be done via allowing , where  and  give the orthogonal eigenvectors and their nonzero eigenvalues, respectively (we know that  is symmetric positive semi-definite since it is the sum of SPSD matrices). Then we know that  will be the optimal solution  (Trefethen and Bau?).<br>However, while this has an optimal solution in closed form, the objective  is not at all well motivated in this context, besides that it will ensure  doesn't have entries diverging to infinity. Perhaps a better solution would be to minimize the Kullback-Leibler divergence, . By definition,<br><br>While I think this is a convex function (the rightmost term is reminiscient of a log-sum-exp), it's not really tractable as an objective even when approximating the expectation with an average over the empirical distribution of . The integral over  is not tractable in closed form. However, examining the role of this term suggests some regularization against higher frequency . This intuition comes from that, if  is highly oscillatory and  is nonzero,  will often be true and thus we will be far from the minimum. In this sense, we approximate the KL as<br><br>For simplicity, imagine  as a diagonal matrix with  scaling with  in some way. While  will clearly have a minimizer at , this is physically meaningless due to the relaxation of the log-sum-exp term, i.e., this will not be guaranteed to minimize any statistical divergence in any manner. However, it seems reasonable to return to the Fisher divergence score-matching setting to say<br><br><br>Generally, the neural networks for diffusion models are minimizing functions of the form<br><br>which approximates the loss<br><br>where  is some density that we prescribe to the "importance" of different time-points and  is the measure of  following the SDE<br><br>with . Suppose now that we consider  with . Then,<br><br>where we abuse notation to say  and  . Finally, suppose we separate the random variables from time in our functions via  Then,<br><br>However, consider that we know . Then, we should have .Polynomial Score Matching<br><br>
<br><a data-href="Map jacobian determinant" href="https://dsharp.dev/thesis/research/transport-intrinsics/map-jacobian-determinant.html" class="internal-link" target="_self" rel="noopener">Map jacobian determinant</a>
<br><a data-href="Max Map Adaptation" href="https://dsharp.dev/thesis/research/transport-intrinsics/max-map-adaptation.html" class="internal-link" target="_self" rel="noopener">Max Map Adaptation</a>
<br><a data-href="Optimizing Linear Maps (new draft)" href="https://dsharp.dev/thesis/research/transport-intrinsics/optimizing-linear-maps-(new-draft).html" class="internal-link" target="_self" rel="noopener">Optimizing Linear Maps (new draft)</a>
<br><a data-href="Polynomial Score Matching" href="https://dsharp.dev/thesis/research/transport-intrinsics/polynomial-score-matching.html" class="internal-link" target="_self" rel="noopener">Polynomial Score Matching</a>
<br><a data-href="Rectified Linear Parameterization" href="https://dsharp.dev/thesis/research/transport-intrinsics/rectified-linear-parameterization.html" class="internal-link" target="_self" rel="noopener">Rectified Linear Parameterization</a>
<br><a data-href="Sigmoid clustering" href="https://dsharp.dev/thesis/research/transport-intrinsics/sigmoid-clustering.html" class="internal-link" target="_self" rel="noopener">Sigmoid clustering</a>
<br>Transport Intrinsics<br><br>
<br><a data-href="Nongaussian KL Expansion" href="https://dsharp.dev/thesis/research/transport-extrinsics/nongaussian-kl-expansion.html" class="internal-link" target="_self" rel="noopener">Nongaussian KL Expansion</a>
<br><a data-href="Parameter estimation" href="https://dsharp.dev/thesis/research/transport-extrinsics/parameter-estimation.html" class="internal-link" target="_self" rel="noopener">Parameter estimation</a>
<br>Transport Extrinsics<br><br><br><br>Suppose we have two probability measures, , and we would like to find the distance between them. While one naturally might consider the Wasserstein metric, which (by Brenier-Benamou) is given by<br><br>this is well-studied and has well-known restrictions. An alternative here would be the Fisher-Rao distance,<br><br>Note that this is distance is constrained by a reaction PDE instead of a continuity equation. We can see that<br><br>where . Since we know , then the total mass can't possibly change (consider discretizing this via Euler and noting that it won't change for any step-size , so it converges as ). One can see that the (adjusted) Fisher-Rao induces a geodesic of the form<br><br>but this is irrelevant to the topic at hand.<br><br>The PDE constraining the Wasserstein-Fisher-Rao metric will intuitively be<br><br>i.e., we use velocity field  and "reaction function"  to govern the dynamics of . For a given functional , we would like to find a "Wasserstein-Fisher-Rao" gradient flow. In particular, we would like to find a velocity field  and reaction function  which determine a  s.t.  minimizes  using constant-speed geodesics in the WFR geometry. To do this, we first must define a "Wasserstein-Fisher-Rao gradient". Generally one defines such gradients by writing  as  for some  governing the PDE (TODO: elaborate?). Observe that<br><br>Then, if I have a measure , I would end up with something like<br><br>See, e.g., <a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2301.01766" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2301.01766" target="_blank">YWR23</a><br><br>At first, we may be interested in the KL-divergence function often used in gradient flows, which (for a given distribution ) has pertinent information<br><br>but if we keep  as a discrete measure, we will not be able to ever estimate . It is possible to use a kernelized version of  in the vein of SVGD, but I'm not sold. Alternatively, I could use score matching. For the record, this gives<br><br>I don't think I need to center the weight derivative? Anyway, if  is truly a discrete distribution, then this is undefined. If you kernelize it.... I'm not entirely sure, but it'd give something I guess? On the other hand, perhaps you could match the score (or score ratio). See <a data-href="Polynomial Score Matching" href="https://dsharp.dev/thesis/research/transport-intrinsics/polynomial-score-matching.html" class="internal-link" target="_self" rel="noopener">Polynomial Score Matching</a>, which would need to be modified for accounting for , which weights the true .<br><br>Suppose we now have<br><br>where  is the norm in the RKHS induced by the kernel . <a data-tooltip-position="top" aria-label="https://akorba.github.io/resources/swslstuttgart2019slides.pdf" rel="noopener" class="external-link" href="https://akorba.github.io/resources/swslstuttgart2019slides.pdf" target="_blank">One can find</a> that<br><br>where  refers to the gradient in the second argument. Suppose that . Then,<br>
. More generally, if  for  positive and  symmetric distance, then you get  which is generally pretty easy to compute. This gives the update equation as<br><br><br>What's nice here is that this is "forgetful" of our initial statue , assuming the convexity and all that (probably not the case, but still). The choice of  is one of preconditioning, it's not an end-all-be-all choice in theory. So far in our transport quadrature, our choice of reference basically entirely constrains the points to be the pushforward of the reference quadrature under the map with no change to the weights, where this isn't necessarily the case.<br><br>Take our Hilbert space  to be polynomials up to degree  with inner product<br><br>so our kernel becomes the Christoffel-Darboux kernel of<br><br>with  the  norm. I.e., this kernel projects onto the polynomials , which are orthogonal w.r.t. . Without loss of generality, assume . Obviously, for any function , we see<br><br>which means the embedding of an arbitrary function  into our RKHS is exactly the orthogonal projection of . Our maximum mean discrepancy becomes<br><br>Assume we want to match points and weights , i.e., discrete measure  to some target . We see<br><br>^3be7e2<br>
with . Then, we get that<br><br>and somewhat less obviously, suppose we abuse notation to consider a diagonal matrix  with diagonal entry  equal to ; matrix calculus dictates<br><br>which indicates the trace is operating on a matrix with only one nonzero column ; therefore,  is the th entry in the th column for  and zero otherwise. This corresponds to<br><br>Here we maintain control of three things: . We know that  induces the orthogonal polynomial family , and this gets normalized (with respect to ), so we don't get to choose which scaling for the family. Notably, for fixed , this becomes a quadratic loss in , so we can use a linear solve to find the solution. For the case of finding  as well, it seems straightforward enough to attempt to just choose  and optimize over these values. Unfortunately, even in one dimension, the Gaussian quadrature being unique does not mean that it will be easy to find in this setting. In higher dimensions, this will be very difficult without something of a greedy procedure.<br><br>We now assume data from  and want to estimate a quadrature via discrete  as  and we have reference distribution . Taking  as our "preconditioner", we estimate  for  from data (perhaps), then we have (for  quadrature points)<br><br>One can compare and contrast this with the minimization problem given in <a data-href="#^021c59" href="https://dsharp.dev/#^021c59" class="internal-link" target="_self" rel="noopener">^021c59</a>. While they are very similar, note that the geometry of the static problem by  on the points, and the WFR places the scaling of  on the weights. This is a slight change, but can make remarkable differences due to imposing different geometries.Wasserstein-Fisher-Rao Gradient Flow<br><br>I'm particularly interested if this can be extended to work for flows of functions and, in particular, orthogonal polynomials. Recall that<br>
<br><br>It's fairly clear that, if you take the inner product of both sides with , we get that<br><br>Similarly,<br>
We can simplify the numerator for  as  due to wikipedia. The second form of the norm does not seem very helpful.<br>Assuming this is correct, we further constrict our polynomials to have unit magnitude, which automatically indicates that  and   by construction. Now suppose we construct  as the th orthonormal polynomial w.r.t. measure . We should get that  is a function representing the th recurrence term at time .<br>One could show that (assuming unit variance of the polynomials)<br><br>I have no idea what  should be due to the representation of . Additionally, note that  and  depend on  and . At this point, this is a disgusting equation whic h no one would ever want to solve. At least I don't want to.<br>Another addition is that this becomes somewhat intractable in higher than one dimension due to the finicky nature of recurrence relations on multivariate polynomials.<br>Finally, I'm not entirely sure what one would linearize here, or how this could possibly be made more efficient.<br><br>The subsequent question is what else you would transport. One idea I like is transporting roots so that, at the end of the transport, you approximate the exact roots of the orthonormal polynomials (then one could maybe approximate the value of the Christoffel function or something? I'm not sure)<br>Alternatively, another idea would be somehow transporting the weights so that you keep the points and the weights change according to the transport (in the vein of tempered importance weighting?)<br><br>Generally, we know , where  and . Because I'm lazy, I'm going to drop the hat (even though technically  doesn't agree with the above definition of ). Then, we see the more complicated time derivative of  as<br><br>Via our abuse of notation, however,  stays the same as <a data-href="#^c0ec95" href="https://dsharp.dev/#^c0ec95" class="internal-link" target="_self" rel="noopener">^c0ec95</a>. On the other hand, we get<br>
<br><br>Suppose we know  up to a constant. Then, set . Then, , so we observe<br><br>and<br><br>Additionally, on the corner cases, assuming  we know<br><br>Now we can formulate an algorithm:<br><br>Assume: , <br>
Input: (possibly unnormalized?) density ,  for , step size <br>
Output:  for , which integrate , respectively<br>
<br>for <br>
- Calculate  from .<br>
- Set   and renormalize <br>
- Note that , so calculate <br>
- for ,<br>
- for each quadrature point <br>
- Calculate <br>
- Calculate <br>
- Calculate <br>
- Estimate $$<br>
\begin{align}<br>
\dot{a}{k}(t) &amp;\approx \sum\limits{i=1}^{N}w{t}^{(i)}x^{(i)}{t}p{k}^{(i)}\left[p{k}^{(i)}v^{(i)} + 2\dot{p}{k}^{(i)}\right]\<br>
\dot{\ell}{k}(t) &amp;\approx \sum\limits{i=1}^{N}w{t}^{(i)}p{k}^{(i)}\left[p{k}^{(i)}v^{(i)} + 2\dot{p}_{k}^{(i)}\right]<br>
\end{align}


<br>Calculate  from .
<br>Note that this approximates the Euler discretization of the ODE of . However, <a data-tooltip-position="top" aria-label="^diff-a-l" data-href="#^diff-a-l" href="https://dsharp.dev/#^diff-a-l" class="internal-link" target="_self" rel="noopener">the inner for loop</a> approximates the derivative of , given the values of , so one could use any other scheme (e.g., Runge-Kutta).Polynomial Recurrence Flow<br><br>Setting the stage<br><br>We are interested in, for , approximating the expectation  by a quadrature rule of the form . There are a few central questions:<br>
<br>What can we prove about  when integrating ?
<br>What can we prove about  when integrating ?
<br>How does the error between  and  affect the difference between  and ?
<br>How do we consider all of these to bound the error on 
<br>How do we choose  to minimize  in some appropriate norm? Does this change how we choose the loss?
<br><br><br>Quadrature generally, to some degree, approximates inner products via polynomials of some kind. Gauss quadrature gets phenomenal convergence via the fact that  is exact on polynomials up to degree . Clenshaw-Curtis is exact on polynomials of degree , and many other quadrature rules share this property. Needless to say, this hinges precisely on the fact that  , when smooth, can be approximated very well by polynomials; further, if  "largely made of" low-degree polynomials (i.e. any polynomial approximation, e.g. PCE, has rapidly decaying coefficients), we know that this quadrature rule works very well. Suppose we have , orthogonal under ; let  be the degree  PCE of . We know that, for sufficiently smooth ,<br><br>which comes from Parseval's identity (see Sagiv 22). What we want is that, given a function  and a set , we can create an approximation<br><br>or something like that. Note here that  is a complete degree of freedom-- we can choose it however we wish. Note here a few things:<br>
<br> is probably not a polynomial, and  is almost certainly not a polynomial
<br>Because of this,  is almost definitely not a polynomial
<br>Regardless, assuming orthonormality of , we get that

Similar to this, we get

Therefore, a spectral projection of  onto  is equivalent to a spectral projection of  onto , and thus we inherit any convergence properties of spectral approximation on  of the function .
<br><br>Suppose we map from the reference  to  via the inverse CDF of the Normal distribution as . Then,  are naturally the Legendre polynomials and  is a Legendre polynomial composed with the CDF of  which, while smooth, is entirely bounded for any given . Therefore, a -point quadrature rule will only capture behavior on the domain for , which grows rather slowly (i.e., the effective domain of this quadrature rule is rather small). However, the basis functions  are fairly smooth so we can get convergence faster than the  rate cited above.<br><br>In the above example, the problem is ostensibly that the transport  maps a distribution with "lighter tails" (in fact, tails that are exactly zero) to a distribution with "heavier tails". It then stands to reason that we might do better if we do the opposite way. Consider now  (the reference is Gaussian now) and , i.e., . We are initially pleased that then  are orthogonal polynomials! However, there are a few problems that crop up.<br>
<br>; so, if we use a quadrature rule with  odd, the middle point index  (which receives the most weight!) will stay at zero, where  Therefore, our "practitioner's method" places a lot of weight where the density places zero weight.
<br>The polynomials  are indeed orthogonal and clearly span ; however, they aren't a "traditional family" since , so the th term is a degree  polynomial. Therefore, if , we will never be able to integrate exactly!
<br>Indeed, the rate of convergence for  is abyssmal. The rate is precisely the rate of decay of the coefficients when projecting  onto  (consider Parseval).
<br>What I take away from these examples is that having "lighter" tails isn't enough; you want the same tails, i.e., the transport  should be exactly linear in the tails. In this sense, we might consider regularizing by the log of the Sobolev norm or something.<br><br>Suppose  is a RKHS with kernel . It would be nice to get something of the following form:<br><br>where  is the discrete distribution using the pushforward of  on a quadrature w.r.t. .<br><br>This "moves the onus onto ", so-to-speak. For a given kernel,  will smooth out the "high moments" of the distributions and only care about the broad strokes of similarity. Then, as long as  broadly captures the features of , we get that the bias of the quadrature is largely dependent on how bad the function is itself. In some sense, this is the "easier" problem to solve, because most convergence of quadrature rules depend on how large the norm of  is in some sobolev sense.<br><br>
<br>Use "heat" to diffuse  into , and then perform transport on  for small (or decreasing) .
<br>If  is our integral and  is our -point approximation of the integral , do something like


<br>Note that if , we have that . Similar if in  (sobolev).
<br>Sagiv, Baptista paper is a very loose bound by strong engine for bounding problems.
<br>Suppose we know Gaussian quadrature points for  and consider a map  that draws a voronoi tesselation on the domain and maps deterministically to these quadrature points. This is an atrocious sampling map, but has wonderful expectation approximation properties if it maps gaussian points to gaussian points (for a specific construction).
<br>Consider distributions with compact support?
<br>Sobolev norm is introduced in statistics to ensure uniqueness of the map 
<br>What is the geometry induced by the MMD? Even less formally, more pedestrian, how do simple functions (e.g. piecewise linear) functions morph under ?

<br>In this sense, can we say anything about, e.g., the trapezoidal rule?


<br>Instead of choosing  to minimize the error, maybe just minimize an upper bound on the error.

<br>Something to do with the Sagiv paper bound.
<br>How to tune ? I personally want to use sobolev regularization, but Amir is a little skeptical


<br>His process:
<br>
<br>Create approximation theory for transformed space

<br>From parseval and spectral approximation, etc (This is where I think sobolev norm kicks in!!)


<br>Look at the error of the expectations of  between target and pushforward to get interesting bound
<br>Use bounds on error of pushforward to encourage construction in the right place
Practitioner's Quadrature]]></description><link>https://dsharp.dev/thesis/thesis-progress.html</link><guid isPermaLink="false">Thesis/Thesis Progress.canvas</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 21:37:41 GMT</pubDate></item><item><title><![CDATA[A Hilbert Space Embedding for Distributions]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/6L9XPB6I" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/6L9XPB6I" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/6L9XPB6I" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/6L9XPB6I" target="_blank">local</a><br><br>start-date:: 2007-01-01<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/smolahilbertspaceembedding2007.html</link><guid isPermaLink="false">Thesis/papernotes/smolaHilbertSpaceEmbedding2007.md</guid><dc:creator><![CDATA[Smola, Alex; Gretton, Arthur; Song, Le; Schölkopf, Bernhard]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:07:20 GMT</pubDate></item><item><title><![CDATA[Analysis of discrete least squares on multivariate polynomial spaces with evaluations at low-discrepancy point sets]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/L26Q72GA" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/L26Q72GA" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/L26Q72GA" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/L26Q72GA" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/E33G75CD/Migliorati%20and%20Nobile%20-%202015%20-%20Analysis%20of%20discrete%20least%20squares%20on%20multivariate.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/E33G75CD/Migliorati and Nobile - 2015 - Analysis of discrete least squares on multivariate.pdf" target="_blank">pdf</a><br><br>start-date:: 2015-08-01<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/miglioratianalysisdiscreteleast2015.html</link><guid isPermaLink="false">Thesis/papernotes/miglioratiAnalysisDiscreteLeast2015.md</guid><dc:creator><![CDATA[Migliorati, Giovanni; Nobile, Fabio]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:07:03 GMT</pubDate></item><item><title><![CDATA[Constructing Least-Squares Polynomial Approximations]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/YRDF2J3G" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/YRDF2J3G" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/YRDF2J3G" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/YRDF2J3G" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/UQNGTEYY/Guo%20et%20al.%20-%202020%20-%20Constructing%20Least-Squares%20Polynomial%20Approximatio.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/UQNGTEYY/Guo et al. - 2020 - Constructing Least-Squares Polynomial Approximatio.pdf" target="_blank">pdf</a><br><br>start-date:: 2020-01-01<br>
end-date::<br>
page-no:: 500<br><br>comment:: <br><br><br><br> <br>]]></description><link>https://dsharp.dev/thesis/papernotes/guoconstructingleastsquarespolynomial2020.html</link><guid isPermaLink="false">Thesis/papernotes/guoConstructingLeastSquaresPolynomial2020.md</guid><dc:creator><![CDATA[Guo, Ling; Narayan, Akil; Zhou, Tao]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:06:58 GMT</pubDate></item><item><title><![CDATA[Matrix Models for Beta Ensembles]]></title><description><![CDATA[<a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---probability" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---probability</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematical-physics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematical-physics</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---representation-theory" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---representation-theory</a> 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/M7B3F5ZS" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/M7B3F5ZS" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/M7B3F5ZS" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/M7B3F5ZS" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/LJJW7TEL/Dumitriu%20and%20Edelman%20-%202002%20-%20Matrix%20Models%20for%20Beta%20Ensembles.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/LJJW7TEL/Dumitriu and Edelman - 2002 - Matrix Models for Beta Ensembles.pdf" target="_blank">pdf</a><br><a href="https://dsharp.dev?query=tag:subject/mathematics---probability" class="tag" target="_blank" rel="noopener">#subject/mathematics---probability</a><br>
<a href="https://dsharp.dev?query=tag:subject/mathematical-physics" class="tag" target="_blank" rel="noopener">#subject/mathematical-physics</a><br>
<a href="https://dsharp.dev?query=tag:subject/mathematics---representation-theory" class="tag" target="_blank" rel="noopener">#subject/mathematics---representation-theory</a><br><br>start-date:: 2002-11-01<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/dumitriumatrixmodelsbeta2002.html</link><guid isPermaLink="false">Thesis/papernotes/dumitriuMatrixModelsBeta2002.md</guid><dc:creator><![CDATA[Dumitriu, Ioana; Edelman, Alan]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:34:46 GMT</pubDate></item><item><title><![CDATA[Neural Spline Flows]]></title><description><![CDATA[<a class="tag" href="https://dsharp.dev/?query=tag:subject/computer-science---machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/computer-science---machine-learning</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/statistics---machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/statistics---machine-learning</a> 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/W6NS74XM" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/W6NS74XM" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/W6NS74XM" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/W6NS74XM" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/83JLEZA8/Durkan%20et%20al.%20-%202019%20-%20Neural%20Spline%20Flows.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/83JLEZA8/Durkan et al. - 2019 - Neural Spline Flows.pdf" target="_blank">pdf</a><br><a href="https://dsharp.dev?query=tag:subject/computer-science---machine-learning" class="tag" target="_blank" rel="noopener">#subject/computer-science---machine-learning</a><br>
<a href="https://dsharp.dev?query=tag:subject/statistics---machine-learning" class="tag" target="_blank" rel="noopener">#subject/statistics---machine-learning</a><br><br>start-date:: 2019-12-02<br>
end-date::<br>
page-no:: 1<br><br>comment:: <br><br><br><br><br>lower triangular <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=2&amp;annotation=BZGECULI" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=2&amp;annotation=BZGECULI" target="_blank">(p. 2)</a><br><br>they can be inverted exactly in a single pass <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=WYKSF3N5" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=WYKSF3N5" target="_blank">(p. 3)</a><br>
<br>logit <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=4ML3QRAQ" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=4ML3QRAQ" target="_blank">(p. 3)</a><br>
<br>In particular, a linear transformation with matrix W is parameterized in terms of its LU-decomposition W = PLU, where P is a fixed permutation matrix, L is lower triangular with ones on the diagonal, and U is upper triangular. By restricting the diagonal elements of U to be positive, W is guaranteed to be invertible. <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=Q78AGYW4" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=3&amp;annotation=Q78AGYW4" target="_blank">(p. 3)</a><br><br>the linear tails <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=4&amp;annotation=GR8KG7H5" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=4&amp;annotation=GR8KG7H5" target="_blank">(p. 4)</a><br>
<br>widths and heights <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=VK2WQWGI" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=VK2WQWGI" target="_blank">(p. 5)</a><br><br>The vector θd i is passed through a softplus function <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=RF8J4EQZ" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=RF8J4EQZ" target="_blank">(p. 5)</a><br>
<br>θ1:d−1 = Trainable parameters <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=AEPQF5YI" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=5&amp;annotation=AEPQF5YI" target="_blank">(p. 5)</a><br>
<br>the number of parameters <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=8&amp;annotation=M4EKK592" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=8&amp;annotation=M4EKK592" target="_blank">(p. 8)</a><br>
<br>s(k) = (y(k+1) − y(k))/w(k) be the slopes <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/83JLEZA8?page=13&amp;annotation=ED5GSV52" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/83JLEZA8?page=13&amp;annotation=ED5GSV52" target="_blank">(p. 13)</a> <br>]]></description><link>https://dsharp.dev/thesis/papernotes/durkanneuralsplineflows2019.html</link><guid isPermaLink="false">Thesis/papernotes/durkanNeuralSplineFlows2019.md</guid><dc:creator><![CDATA[Durkan, Conor; Bekasov, Artur; Murray, Iain; Papamakarios, George]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:06:45 GMT</pubDate></item><item><title><![CDATA[On minimax density estimation via measure transport]]></title><description><![CDATA[<a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---probability" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---probability</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---statistics-theory" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---statistics-theory</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/statistics---machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/statistics---machine-learning</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/62g07" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/62g07</a> 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/LCFHSUIX" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/LCFHSUIX" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/LCFHSUIX" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/LCFHSUIX" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/44JW6BW7/Wang%20and%20Marzouk%20-%202022%20-%20On%20minimax%20density%20estimation%20via%20measure%20transpor.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/44JW6BW7/Wang and Marzouk - 2022 - On minimax density estimation via measure transpor.pdf" target="_blank">pdf</a><br><a href="https://dsharp.dev?query=tag:subject/mathematics---probability" class="tag" target="_blank" rel="noopener">#subject/mathematics---probability</a><br>
<a href="https://dsharp.dev?query=tag:subject/mathematics---statistics-theory" class="tag" target="_blank" rel="noopener">#subject/mathematics---statistics-theory</a><br>
<a href="https://dsharp.dev?query=tag:subject/statistics---machine-learning" class="tag" target="_blank" rel="noopener">#subject/statistics---machine-learning</a><br>
<a href="https://dsharp.dev?query=tag:subject/62g07" class="tag" target="_blank" rel="noopener">#subject/62g07</a>,-62g20<br><br>start-date:: 2022-09-19<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/wangminimaxdensityestimation2022.html</link><guid isPermaLink="false">Thesis/papernotes/wangMinimaxDensityEstimation2022.md</guid><dc:creator><![CDATA[Wang, Sven; Marzouk, Youssef]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:09:22 GMT</pubDate></item><item><title><![CDATA[Optimal weighted least-squares methods]]></title><description><![CDATA[<a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---numerical-analysis" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---numerical-analysis</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---probability" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---probability</a> <a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---statistics-theory" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---statistics-theory</a> 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/NP7LY25W" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/NP7LY25W" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/NP7LY25W" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/NP7LY25W" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/3WUGCQF7/Cohen%20and%20Migliorati%20-%202016%20-%20Optimal%20weighted%20least-squares%20methods.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/3WUGCQF7/Cohen and Migliorati - 2016 - Optimal weighted least-squares methods.pdf" target="_blank">pdf</a><br><a href="https://dsharp.dev?query=tag:subject/mathematics---numerical-analysis" class="tag" target="_blank" rel="noopener">#subject/mathematics---numerical-analysis</a><br>
<a href="https://dsharp.dev?query=tag:subject/mathematics---probability" class="tag" target="_blank" rel="noopener">#subject/mathematics---probability</a><br>
<a href="https://dsharp.dev?query=tag:subject/mathematics---statistics-theory" class="tag" target="_blank" rel="noopener">#subject/mathematics---statistics-theory</a><br><br>start-date:: 2016-08-01<br>
end-date::<br>
page-no:: 6<br><br>comment:: <br><br><br><br><br> <br>]]></description><link>https://dsharp.dev/thesis/papernotes/cohenoptimalweightedleastsquares2016.html</link><guid isPermaLink="false">Thesis/papernotes/cohenOptimalWeightedLeastsquares2016.md</guid><dc:creator><![CDATA[Cohen, Albert; Migliorati, Giovanni]]></dc:creator><pubDate>Thu, 18 Jan 2024 14:18:32 GMT</pubDate></item><item><title><![CDATA[Sample Complexity of Sinkhorn divergences]]></title><description><![CDATA[<a class="tag" href="https://dsharp.dev/?query=tag:subject/mathematics---statistics-theory" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#subject/mathematics---statistics-theory</a> 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/HUNBQZD3" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/HUNBQZD3" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/HUNBQZD3" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/HUNBQZD3" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/HIV9IKVG/Genevay%20et%20al.%20-%202019%20-%20Sample%20Complexity%20of%20Sinkhorn%20divergences.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/HIV9IKVG/Genevay et al. - 2019 - Sample Complexity of Sinkhorn divergences.pdf" target="_blank">pdf</a><br><a href="https://dsharp.dev?query=tag:subject/mathematics---statistics-theory" class="tag" target="_blank" rel="noopener">#subject/mathematics---statistics-theory</a><br><br>start-date:: 2019-02-05<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/genevaysamplecomplexitysinkhorn2019.html</link><guid isPermaLink="false">Thesis/papernotes/genevaySampleComplexitySinkhorn2019.md</guid><dc:creator><![CDATA[Genevay, Aude; Chizat, Lénaic; Bach, Francis; Cuturi, Marco; Peyré, Gabriel]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:06:24 GMT</pubDate></item><item><title><![CDATA[Scattered Data Approximation]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/W8NRK3AX" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/W8NRK3AX" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/W8NRK3AX" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/W8NRK3AX" target="_blank">local</a><br><br>start-date:: 2004-12-13<br>
end-date::<br>
page-no:: <br><br>comment:: <br><br><br>
<br>]]></description><link>https://dsharp.dev/thesis/papernotes/wendlandscattereddataapproximation2004.html</link><guid isPermaLink="false">Thesis/papernotes/wendlandScatteredDataApproximation2004.md</guid><dc:creator><![CDATA[Wendland, Holger]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:20:14 GMT</pubDate></item><item><title><![CDATA[Spectral Representation and Reduced Order Modeling of the Dynamics of Stochastic Reaction Networks via Adaptive Data Partitioning]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="http://zotero.org/users/6145110/items/CH3NF4VV" rel="noopener" class="external-link" href="http://zotero.org/users/6145110/items/CH3NF4VV" target="_blank">online</a> <a data-tooltip-position="top" aria-label="zotero://select/library/items/CH3NF4VV" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://select/library/items/CH3NF4VV" target="_blank">local</a> <a data-tooltip-position="top" aria-label="file:///Users/dannys4/Zotero/storage/KD9ZTYY6/Sargsyan%20et%20al.%20-%202010%20-%20Spectral%20Representation%20and%20Reduced%20Order%20Modeling.pdf" rel="noopener" class="external-link" href="https://dsharp.dev/file://Users/dannys4/Zotero/storage/KD9ZTYY6/Sargsyan et al. - 2010 - Spectral Representation and Reduced Order Modeling.pdf" target="_blank">pdf</a><br><br>start-date:: 2010-01-01<br>
end-date::<br>
page-no:: 4401<br><br>comment:: <br><br><br><br>spectral methods to properly represent the latter <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4401&amp;annotation=GNQFCVKQ" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4401&amp;annotation=GNQFCVKQ" target="_blank">(p. 4401)</a><br>
<br>ξ <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4402&amp;annotation=RD36B3P2" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4402&amp;annotation=RD36B3P2" target="_blank">(p. 4402)</a><br>This map, denoted by the shorthand notation η = R(ξ), is called the Rosenblatt transformation [39]. Note that the Rosenblatt transformation is not unique: by ordering the ξi’s in different ways, one can obtain L! different sets of uniform random variables. <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=YXUJRQBD" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=YXUJRQBD" target="_blank">(p. 4403)</a><br>
<br>Instead, in this work it is estimated by sampling ξ using a standard kernel density estimator (KDE); see [43, 42] <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=RPGSFJYW" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=RPGSFJYW" target="_blank">(p. 4403)</a><br>The KDE estimate of its joint PDF is a sum of N multivariate Gaussian functions centered at each data point ξ(n): <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=LDQDV29M" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4403&amp;annotation=LDQDV29M" target="_blank">(p. 4403)</a><br>
<br>allowing an efficient computation scheme. <a data-tooltip-position="top" aria-label="zotero://open-pdf/library/items/KD9ZTYY6?page=4404&amp;annotation=KGDPMS57" rel="noopener" class="external-link" href="https://dsharp.dev/zotero://open-pdf/library/items/KD9ZTYY6?page=4404&amp;annotation=KGDPMS57" target="_blank">(p. 4404)</a><br>
 <br>]]></description><link>https://dsharp.dev/thesis/papernotes/sargsyanspectralrepresentationreduced2010.html</link><guid isPermaLink="false">Thesis/papernotes/sargsyanSpectralRepresentationReduced2010.md</guid><dc:creator><![CDATA[Sargsyan, Khachik; Debusschere, Bert; Najm, Habib; Maître, Olivier Le]]></dc:creator><pubDate>Mon, 19 Feb 2024 22:07:12 GMT</pubDate></item><item><title><![CDATA[Couri 2014?]]></title><description><![CDATA[ 
 <br>
<br>Create GCP

<br>create 

<br>Here,  is a projection of the point y onto the convex contraints ,  is gradient at 


<br>Find  and set ,  such that both are true:

<br> ( is param,  is part of alg) AND
<br>




<br>These can be interpreted as:<br>
<br> is the distance to closest point in domain to gradient descent of size t
<br>is the amount of descent the Generalized Cauchy point goes
<br> is the Generalized Cauchy Point

<br>Cauchy step is within the trust region
<br>Cauchy step descends further than some scaling of how similar  and  are.


<br>Then,  must satisfy ONE of the following

<br>, where 
<br>


<br>These can be interpreted as:

<br>If there is a  "worse" Cauchy step (measured by the second criterion above) up to constant 
<br> makes a large enough jump compared to  and, if  is sufficiently small, then , i.e., the side of the gradient puts us within some neighborhood of the edge of trust region
<br>As required in paper, 
<br>If we can find such a , then we try much less hard to make it worse?
<br>We make a step that, in practice, moves us mildly closer to the boundary but not TOO close?


]]></description><link>https://dsharp.dev/thesis/research/paper-algorithms/couri-2014/couri-2014.html</link><guid isPermaLink="false">Thesis/Research/Paper Algorithms/Couri 2014?.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Thu, 16 May 2024 17:57:20 GMT</pubDate></item><item><title><![CDATA[Smolyak]]></title><description><![CDATA[ 
 <br>Input: multi-index set mset_full, which is assumed downward-closed
Output: quad_rules: set of tensor-product quadratures and their (possibly negative) associated cardinality `Vector{Tuple{Midx, Int}}`
keep_looping = true
quad_rules = []

while keep_looping
	# Form the mset for this loop
	look at indexes of occurrences that are not 1.
	form them into mset_loop
	get reduced frontier frontier of mset_loop
	
	# Adjust each frontier member and reindex their ancestors' number of occurrences
	for idx_midx_loop in frontier:
		# Access the adjustment for this frontier member
		get index idx_midx_full corresponding to the multi-index idx_midx_loop in mset_full
		set j = occurrences[idx_midx_full] - 1, e.g., if idx_midx_full occurs $j+1=-1$ times, then $j=-2$.
		occurrences[idx_midx_full] = 1

		# Reindex the number of occurrences of the backward ancestors
		get backward ancestors of idx_midx_loop
		for each ancestor idx_back_loop of idx_midx_loop
			get original index idx_back_full of idx_back_loop (i.e. index of idx_back_loop w.r.t. mset_full, not mset_loop)
			occurrences[idx] -= j
		end for
		push (idx_midx_full, j) onto quad_rules
	end for
	keep_looping = occurrences is not identically 1
end while

return quad_rules
Copy]]></description><link>https://dsharp.dev/thesis/research/paper-algorithms/smolyak.html</link><guid isPermaLink="false">Thesis/Research/Paper Algorithms/Smolyak.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 18:40:39 GMT</pubDate></item><item><title><![CDATA[Transport-based Quadrature]]></title><description><![CDATA[ 
 <br>See <a data-href="Sagiv, Feb 14 24" href="https://dsharp.dev/logistics/meetings/sagiv,-feb-14-24.html" class="internal-link" target="_self" rel="noopener">Sagiv, Feb 14 24</a> for initial background.<br><br>
<br>MMD: Check out <a data-href="smolaHilbertSpaceEmbedding2007" href="https://dsharp.dev/thesis/papernotes/smolahilbertspaceembedding2007.html" class="internal-link" target="_self" rel="noopener">smolaHilbertSpaceEmbedding2007</a> for MMD bounds
<br>Diffuse  using "heat": Alternatively, consider using entropic regularization as seen in OT <a data-href="genevaySampleComplexitySinkhorn2019" href="https://dsharp.dev/thesis/papernotes/genevaysamplecomplexitysinkhorn2019.html" class="internal-link" target="_self" rel="noopener">genevaySampleComplexitySinkhorn2019</a>
<br>Sobolev norm introduced for uniqueness of : For some Sob. spaces, solution is strongly connected to kernel-based interpolation

<br>Not sure what this means
<br>In Sven's paper <a data-href="wangMinimaxDensityEstimation2022" href="https://dsharp.dev/thesis/papernotes/wangminimaxdensityestimation2022.html" class="internal-link" target="_self" rel="noopener">wangMinimaxDensityEstimation2022</a>, they actually don't use Sobolev norms for uniqueness (KR map should be unique, no?)


<br>How does  morph "simple" functions: There might be results on trapezoidal rule and MMD ? 
<br>"Tune" function class  with sob. reg? This is not "scary" (?) see <a data-href="wendlandScatteredDataApproximation2004" href="https://dsharp.dev/thesis/papernotes/wendlandscattereddataapproximation2004.html" class="internal-link" target="_self" rel="noopener">wendlandScatteredDataApproximation2004</a>, (I believe chapter 10)
<br><br>I believe that DPP can play a role to tackle the problem. Some DPP-based quadratures are nothing but a probabilistic relaxation of Gaussian quadrature. See <a data-href="dumitriuMatrixModelsBeta2002" href="https://dsharp.dev/thesis/papernotes/dumitriumatrixmodelsbeta2002.html" class="internal-link" target="_self" rel="noopener">dumitriuMatrixModelsBeta2002</a> <br>
<br>I'm entirely unclear what the edelman paper has to do with this--there's nothing on DPPs here? There is some semblance of quadrature for sure, though.<br>
With DPPs, we gain universality; higher-dimension + arbitrary domains + strong theoretical guarantees. Some questions related to the numerical simulation of DPPs are still open. See Belhadji 2020, Belhadji 2019, Guillame 2020
<br><br>Again, see <a data-href="Sagiv, Feb 14 24" href="https://dsharp.dev/logistics/meetings/sagiv,-feb-14-24.html" class="internal-link" target="_self" rel="noopener">Sagiv, Feb 14 24</a> for reference notation. Suppose we have our target orthonormal basis  constructed from transport map  and reference polynomials . Then, we know that, if  is such that , we must have that . In a similar sense, we actually have that  cannot have any roots besides this. Now, consider the set  where  is the th largest root of . We consider the set of lagrange interpolants  . We know that if , for , we get the Gaussian quadrature rule from fixing  and getting that<br><br>Then, the natural extension for general  should be<br><br>Therefore, the answer is indeed to keep the same points but adjust the weights to this function. Note that , which generally will not be a polynomial; therefore, this shouldn't reduce to the traditional Gaussian quadrature weights. In this setting, one could theoretically compute these "offline" integrals via brute force (e.g. MC), or alternatively via a gaussian quadrature on .<br>Supposing  is indeed a polynomial map of degree , however, we still have no expectation that these weights should match up against the Gaussian weights; we end up "embedding" degree  polynomials into the larger space of degree  polynomials (the degrees are multiplied due to the composition of polynomials). This might be worth investigating as a sub-point of <a data-href="#^40a780" href="https://dsharp.dev/#^40a780" class="internal-link" target="_self" rel="noopener">^40a780</a>.<br><br>Suppose instead that we are interested in exactly integrating functions of the form . For any choice of , then we must have, for any choice of <br><br>Since this must hold for any , it must hold for each of  (i.e., elementary vectors with all zeros except for a 1 in the th position). Therefore, we get that  for all , and we must solve . However, for the particular choice of  roots of , we know , where  are roots of . Then, the solution of this becomes . Therefore, since the linear system here is nonsingular and we know that the gauss quadrature weights must satisfy this, the weights must be identical to those in Gauss quadrature. The real question, then, becomes-- which is more reasonable ("Efficient", "consistent")? Being exact on basis  wrt ? Or being exact on polynomials wrt ? As  becomes increasingly nonlinear, these two might (or will?) increasingly separate. However, it might be worth looking into or comparing this choice here with DPP-based methods, which have probabilistic guarantees akin to Gaussian quadrature given a DPP on  (i.e. non polynomials).]]></description><link>https://dsharp.dev/thesis/research/quadrature/transport-based-quadrature.html</link><guid isPermaLink="false">Thesis/Research/Quadrature/Transport-based Quadrature.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Tue, 20 Feb 2024 19:37:11 GMT</pubDate></item><item><title><![CDATA[Nongaussian KL Expansion]]></title><description><![CDATA[ 
 <br><br>Suppose we have a scalar-valued stochastic process  for  and , where  describes our physical domain (spatial + temporal) and  describes the sample space. In other words, suppose  for some symmetric positive definite operator  and  for some . Making some relatively mild assumptions about , i.e., that  is compact and bounded, we can see the KL expansion as<br><br>where we can define<br><br>Then, by construction,<br><br>which simply comes from the orthonormality of . Suppose further that  is a Gaussian process (GP) (i.e., any collection of  gives   s.t.  are jointly Gaussian). Then, one can prove that , meaning we can easily sample from  given the eigenvalues and eigenfunctions (see the Nystrom method).<br>It suffices to show that  are jointly Gaussian (identity covariance and zero mean give the exact shape). This loosely comes from the fact that if you use a convergent scheme to converge the integral in <a data-href="#^b94337" href="https://dsharp.dev/#^b94337" class="internal-link" target="_self" rel="noopener">^b94337</a> , each step will be a sum of Gaussian variables, which itself is Gaussian.<br><br>The problem arises when  is not a GP; we of course maintain zero mean and identity covariance, but the higher moments will become nontrivial (and thus the joint distribution of  nongaussian). We can express the covariance function as<br><br>and consider  the  term truncation of such a series. We can express  as the leading eigenfunction of . I believe, though obviously this isn't rigorous, that this implies a certain "triangular" nature of , in that they are well-ordered in the Knothe-Rosenblatt sense.<br>Suppose we, for the time being, can perform the integrals in <a data-href="#^992e28" href="https://dsharp.dev/#^992e28" class="internal-link" target="_self" rel="noopener">^992e28</a>,<a data-href="#^b94337" href="https://dsharp.dev/#^b94337" class="internal-link" target="_self" rel="noopener">^b94337</a> analytically (or, at the very worst, "close enough"), and thus can find the eigenfunctions and -values. Therefore, for a given set of samples  of the stochastic process, we can obtain samples  with  where  is our truncation level of the KL expansion and  for some unknown distribution . We can calculate an acceptable  before any discussion of how to describe  using samples; the acceptable truncation error exactly comes from the decay of the covariance kernel spectrum.<br>If we could sample  exactly, we could create samples of  "easily" and "cheaply". The problem comes down to approximating the distribution of . In practice, we generally have a stochastic process  where  is a stochastic input to the process (consider, e.g., random diffusivity or initial condition, where  is the solution to a PDE). More on this in <a data-href="#On stochastic inputs" href="https://dsharp.dev/#On_stochastic_inputs" class="internal-link" target="_self" rel="noopener">On stochastic inputs</a>.<br>Suppose, though, we only have access to samples of the solution (and no knowledge/desire for any stochastic inputs). Then, we create an invertible map  such that , i.e., we find a transport map where . If we can find such a map from the samples , then we can sample from  via the relationship  for !<br>From there, we can sample new approximate solutions<br><br><br>If we truly have , where  is a deterministic function of  and random variable  following distribution , then there must exist a deterministic mapping   , since  must then be deterministic for any given realization of . In a distributional sense, we could create a map  such that the operator  satisfies . This is what is traditionally done in our previous triangular transport work. However, this is not well-posed. We know for certain that  is a degenerate distribution! There's only one possible mapping from a choice of input  to a given KL sample .<br>What to do? A few options, I suppose.<br>
<br>Find a map  such that . Note that this is ill-posed; the measures can be equivalent even when the samples get transported poorly. For example, suppose the input is a coin-flip on 0 1 and the output is a coin flip on 1 2. There's two ways of transporting these distributions, but one of them will guarantee you're always wrong.
<br>Find the map  as above such that . This totally disregards interpolation and just matches points (though I suppose, with enough parameters, you could pass overfitting?)
<br>Use something like entropically-regularized OT? Idk enough to say much. Would I need the input/output to match?
<br>Suppose that  is in fact still stochastic for fixed , i.e.,  doesn't capture all of the randomness. Then, the approach described by  is perfectly well-posed, I think?
<br><br><br>We observe that the high order stochastic modes of the KLE are increasingly Gaussian (or at least log-convex looking). Why is that? Is there any reason the bimodal behavior shouldn't get pushed out to parameter number 28? The spectral decay is particularly fast, so this isn't altogether unreasonable to assume that, since the KLE hinges on everything being L2, and the Gaussian is the pretty reasonable choice of measure on L2, it tends to look log-convex or something like that?<br><br>The case studied in the siam UQ presentation is a reaction with bimodal output concentration. It seems entirely reasonable to then consider a stochastic process source term, for example, which can be represented via the KLE, and we use the diffusion equation, e.g.<br><br>Here,  may or may not be stochastic (undecided). Now suppose we represent  spectrally and  as a KLE, i.e.<br><br>Then, consider  such that  is the given inner product here (obviously, this is an infinite-dimensional matrix). This can be computed offline to a fixed  and . Since we are interested in finding  and we draw , then we just solve<br><br>Where  are all known a priori, i.e., offline. Each time we would like to sample , however, this would require a new matrix solve. But if  is factorized a priori (via, e.g., SVD), then the "online" cost should be on the order of a few matrix multiplications.<br><br>Suppose now that  and further that . Then, . We get that<br><br>Now we examine the inner product to note that<br><br>Here, though, we can precisely recover that this has a strong relationship to the Fourier series! <br><br>
<br>Problem doesn't actually reduce here?  The fact this is spectral doesn't make a huge difference
<br>See traditional spectral methods: Learning intrusively via  would be the goal.
<br>I don't personally like this-- training  and  simultaneously seems... dissatisfying
<br>Further Thoughts<br>
<br>What's the problem with spectral UQ? Number of basis functions to solve for gets prohibitively large in high dimension<br>
Suppose we just do a spectral expansion of  with linear differential operator  with random coefficients<br>
Practically, we end up solving

If all  are uncorrelated or something, then we can solve each equation separately via inner product with  and find the  accordingly. The problem, however, is
<br>
<br>What if we don't know the distribution of the input  (i.e. we don't know what basis functions to use)
<br>Even if we did know , if it's high-dimensional, we're out of luck (it needs to be a projection in many dimensions and the coefficients explode exponentially in the dimension)
]]></description><link>https://dsharp.dev/thesis/research/transport-extrinsics/nongaussian-kl-expansion.html</link><guid isPermaLink="false">Thesis/Research/Transport Extrinsics/Nongaussian KL Expansion.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Mon, 06 May 2024 20:47:10 GMT</pubDate></item><item><title><![CDATA[Parameter estimation]]></title><description><![CDATA[ 
 <br>Suppose we have the model<br>
where  indexes the functions we care about. We are generally interested in . One idea here is, for fixed  (which will be disregarded below), we can follow Spantini and create<br>
 and  then create samples  . Then, we create a map  from these samples to a reference and form the composition . When such an algorithm is performed recursively, this will approximately sample the distribution .<br>However, we may not know our . Then, everything becomes more difficult. We may, in principle, want to approximate  but oftentimes  is just some nuisance parameter (sometimes physical, like viscosity, or sometimes a nonphysical parameter to our model). In a frequentist sense of the nuisance case, we now would have  for some unknown . The parameter, as mentioned above, indexes the functions we care about and so we, in principle, now have an infinite number of distributions we may care about. Contrast this with how  indexes , where there's really some particular  we can actually observe which will clearly point us toward one particular distribution.<br>For more of discussion of a state-augmentation approach, see my thesis.<br>Another idea would be to form a map  given each , our th estimate of  at timestep . Then, we can do exactly as above to get an ensemble  which is our analysis ensemble given our prior parameter estimate. Getting the posterior-predictive average  , we now have pairs . We should finally be able to calculate a map  from these samples and then perform . A few notes<br>
<br>We then have to perform a proper  analysis step using the "correct" . These then will properly come from .
<br>We need  to be associated with the distribution . I qualitatively like the average (it seems nice!), but another very reasonably idea would be to just take a sample . 
<br>There's a statistic that we, in some sense, disregard here: , the parameters of . It would be extraordinarily curious to instead perform inference on the pair .

<br>Then, , where . 


<br><br>Flipping some notation, suppose we have a joint distribution  (i.e., static problem, no observations). Consider the idea that  parameterizes the likelihood  (again, in a somewhat frequentist sense). Then, we may end up sampling , i.e., have an ensemble of samples of  for each choice of  (representing a distribution). We may want to perform some kind of generative modeling (or surrogate) approach of learning  from these samples. One method would just be to throw out all but one  for each  (i.e. just take joint samples ), but this wastes precious information. Another idea, perhaps a little more "out-there" would be to learn maps  learned from  mapping to a Gaussian distribution (where every  is parameterized identically). Then, each map  would be parameterized by some vector . In some sense, this is ideally some kind of summary statistic of a distribution. Then, we have pairs . If we learn the map  (not necessarily the same parameterization as ) mapping to a Gaussian, then we would be able to generate samples  for a given parameter to generate realizations of , which in turn would allow us to plug into our  and generate realizations of .]]></description><link>https://dsharp.dev/thesis/research/transport-extrinsics/parameter-estimation.html</link><guid isPermaLink="false">Thesis/Research/Transport Extrinsics/Parameter estimation.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Mon, 06 May 2024 20:47:24 GMT</pubDate></item><item><title><![CDATA[Transport Extrinsics]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="Nongaussian KL Expansion" href="https://dsharp.dev/thesis/research/transport-extrinsics/nongaussian-kl-expansion.html" class="internal-link" target="_self" rel="noopener">Nongaussian KL Expansion</a>
<br><a data-href="Parameter estimation" href="https://dsharp.dev/thesis/research/transport-extrinsics/parameter-estimation.html" class="internal-link" target="_self" rel="noopener">Parameter estimation</a>
<br>]]></description><link>https://dsharp.dev/thesis/research/transport-extrinsics/transport-extrinsics.html</link><guid isPermaLink="false">Thesis/Research/Transport Extrinsics/Transport Extrinsics.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 19:09:52 GMT</pubDate></item><item><title><![CDATA[Map jacobian determinant]]></title><description><![CDATA[ 
 <br><br>I was thinking that there's some result about the integral of the map jacobian determinant. Here's some work<br><br>This bottom ? approximation is just wrong. Nothing I can do here unfortunately<br><br>When looking at the pullback of a map, there are two terms, namely<br><br>But what makes  "look like" ? Consider two scenarios<br>
<br>, or possible, it has tails that are extremely heavy. Consider  as the normal distribution and  for some small . This will be basically  for a large swath of time. On the other hand,  will look like a Gaussian PDF (plus a constant)! In this setting, then,  only really matters in the tails to ensure that the constant  doesn't end up mattering all too much.<img alt="2024_01_25_jacdet_expr.png" src="https://dsharp.dev/lib/media/2024_01_25_jacdet_expr.png">
<br>On the other hand, suppose . Then, basically, the transport map doesn't do anything. In some sense, this comes from a "divergence-free" transport. One can imagine this as a rotation (though rotations are only a tiny subclass of these). This will happen if , via a hand waving at the below


]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/map-jacobian-determinant.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Map jacobian determinant.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Mon, 06 May 2024 20:46:22 GMT</pubDate><enclosure url="https://dsharp.dev/lib/media/2024_01_25_jacdet_expr.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://dsharp.dev/lib/media/2024_01_25_jacdet_expr.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Max Map Adaptation]]></title><description><![CDATA[ 
 <br>
<br>Get multivariate output
<br>Select two output dimensions
<br>Apply marginal gaussian cdf to get samples in unit square
<br>divide into n bins along each axis
<br>Get n^2 "observed" histograms (i.e. counts in each cell)
<br>Get n marginal histograms
<br>Calculate "expected" histogram from product of marginals (i.e. expected counts in each cell)
<br>calculate G-test statistic from difference of "expected" and "observed"
<br>What Midx's to add?<br>
<br>If marginal is not gaussian, add degree to diagonal component
<br>If joint is not independent, add elements of the dimensional reduced margin to lower component

<br>The reduced margin in dimension  of  is going to be elements of the set  where  is the current accepted mset.
<br>if the output of  and  have non-diagonal correlation, then pick  elements of  .
<br>Which elements to pick for indices ? Order them heuristically via the following evaluation of :

<br>If  for some tolerance  (default 1) then just return 
<br>Let  and similar for  . If  then return . 
<br>Finally, return  to break any ties




<br>Assuming  dimensions, we now have up to  added multiindices for the diagonal, plus up to  indices for each pair of outputs (which is ). For  and , this is just under 8000 indices, so we probably don't want to take all of them.
<br>From these, we choose  multi-indices to actually add. We do a similar choice of sorting heuristic as above. Here is the new definition of 

<br>If both  and  are empty except for their final element (i.e., they correspond to "diagonal" multi-indices), return . If they have identical diagonal elements, check if  is shorter than  (recall that they can be multi-indices corresponding to different map outputs)
<br>If   is diagonal and  is not, return  for fixed  (default 2),  i.e., allow the diagonal indices to grow to be a bit larger than cross-term indices.
<br>Same concept in the opposite scenario
<br>If  for some tolerance  (default 1) then just return 
<br>If  return 
<br>Return whichever has the earliest nonzero index (e.g., ), which encodes the idea that the "most complicated" inputs are going to be the first ones.


<br><br>I want a more principled way of ordering these things, and I want to take advantage of <a data-href="Optimizing Linear Maps (new draft)" href="https://dsharp.dev/thesis/research/transport-intrinsics/optimizing-linear-maps-(new-draft).html" class="internal-link" target="_self" rel="noopener">Optimizing Linear Maps (new draft)</a> . Something of the following structure shakes out. Here, I use the semicolon to denote "Vertical stacking" of matrices/vectors.<br>
Input: Starting multi-index set , samples , number of coefficients to add per iteration .<br>
0) Get input of  use basis from  which are, respectively, evaluations of the off-diagonal functions, evaluations of the on-diagonal functions, and the evaluations of the diagonal derivatives. Get  from these matrices and optimize for .<br>
<br>For 
<br>Get Candidate multi-indices ,  to add via Max map adaptation
<br>Compute  which are evaluations of the respective functions for the basis functions corresponding to the basis functions for ,  .
<br>Compute  and  
<br>Get indices for top  values of combined vector , and split these indices into  (note that one or both of these may be empty).
<br>Create , , and  where  for a given matrix  is the columns of  corresponding to the elements of the index set .
<br>Calculate  and create optimal .
]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/max-map-adaptation.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Max Map Adaptation.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Wed, 14 Feb 2024 18:12:04 GMT</pubDate></item><item><title><![CDATA[Optimizing Linear Maps (new draft)]]></title><description><![CDATA[ 
 <br><br>As established in (prev section), choosing a linear map parameterization permits not just more efficient map evaluation, but a more efficient map optimization routine. Recall for a general (not necessarily separable) map, each map component function  can be defined as<br><br>where  are the th basis functions of map component , associated with the nonmonotone and monotone terms, respectively. Further,   are the   multivariate basis functions for the nonmonotone functions (i.e., ). The function and  is the "monotone function"; in the rectified integrated basis, this is the rectified integral, and in the separable linear basis this is the linear combination of, e.g., iRBF functions in . The vectors  and  are the coefficients of each of these respective collections basis functions (technically,  can be generic nonlinear parameters). For maps from samples, we recall the following optimization objective for :<br><br>Substituting Equation <a data-href="#^9e08c8" href="https://dsharp.dev/#^9e08c8" class="internal-link" target="_self" rel="noopener">^9e08c8</a> into this expression, we obtain<br><br>We note that  maps vectors to vectors, so the dot products with its coefficient vectors map these to scalars. We now perform a few simplifications; first, we recognize that the partial derivative  will eliminate the term  by construction of our "nonmonotone" part of our function . Further, we can recognize that a sum of squares of a linear combination is indeed going to be a squared two-norm of a vector. Therefore, we can abuse notation to assemble the matrix  where the  component is evaluation of the  th basis function on  of the respective function and the vector  where the th component is the evaluation of  at point . This simplifies notation into<br><br>This gives way to the astounding realization that, when optimizing for the coefficients  and , the nonquadratic (i.e., log) term entirely independent of . As such, any optimal choice of coefficients  for the nonmonotone the basis  (where the hat indicates that these coefficients are optimal) must satisfy<br><br>This of course leads to a typical solution of an ordinary least squares problem, seen often in numerical linear algebra and linear regression. Therefore, given a choice of monotone coefficients , we know the optimal choice   takes the form<br><br>which comes from any standard text on numerical linear algebra (e.g., Trefethen and Bau).<br><br>We can investigate some alternatives and a more general case. Consider the more general loss function<br><br>We define the function  because, since this is independent of , it can only simplify in the linear case. This gives us the gradient of the objective on the nonmonotone parameters via<br><br><br><br>While there is only one scaling parameter here, , this can easily be extended to when  and  might have different regularization terms (indeed, it could even extend to when we wish to weight different elements of each coefficient vector differently). Suppose  has SVD . Then, setting of  w.r.t.  to zero, we get<br><br>^d8eb8d<br>
We know that  where  .  If , then we know , and the solution becomes clear. If , then we now have an underdetermined set of equations. However, it is clear our goal is to compute  with a minimal norm and so a  "least norm" solution becomes <br><br>where  is the set of all  satisfying <a data-href="#^930584" href="https://dsharp.dev/#^930584" class="internal-link" target="_self" rel="noopener">^930584</a>. Since  has linearly independent columns, we use the Moore-Penrose pseudoinverse to get that<br><br>which agrees with the case of when . Then, the optimization objective becomes<br><br>The regularization term simplifies marginally due to the fact that we know  for any  due to properties of orthogonal matrices. At this point, for general , the logarithmic and regularization terms cannot simplify substantially, so we focus on the first term, i.e. squared-norm loss. We can deduce via clear linear algebra that this ends up becoming<br><br>Note that, in the case of ,  , the orthogonal projector that looks eerily similar to the QR case, and , which gives an equivalent answer to using  , which gives the original least-squares/least-norm solution for .<br>In the linear case, we know that , which simplifies our work substantially to find that the loss function becomes<br><br><br>I think at this point the factorizations are pointless; since I'm only using these solutions once, it ends up becoming significantly more expensive than just using a symmetric solver. Suppose a linear basis and define<br><br>and let  . Then,<br><br>which agrees with all above formulations. This loss will clearly have a gradient (with respect to ) of<br><br>In the adaptive transport map algorithm, we care about what the gradient of  is with respect to coefficients we have yet to introduce into our model; suppose we have candidate basis functions represented by coefficients  and , which correspond to candidate terms  and  for some sets of basis functions  and . Vitally, these sets do not necessarily have the same cardinality, i.e.,  and . Then suppose we have matrices   and vectors  such that<br><br>The loss function, for general  and  and optimal choice  becomes<br><br>Now we differentiate with respect to  and  and evaluate when these coefficients are zero. Starting with the easier case of ,<br><br>Therefore, all we need are the matrices  which are evaluations of the candidate functions at the same points as  and  were evaluated on (i.e. we do not necessarily need evaluations of already "accepted" basis functions).]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/optimizing-linear-maps-(new-draft).html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Optimizing Linear Maps (new draft).md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Mon, 06 May 2024 20:46:46 GMT</pubDate></item><item><title><![CDATA[Rectified Linear Parameterization]]></title><description><![CDATA[ 
 <br><br>Currently, we use functions of the following form to represent a transport map component: given a multi-index set , we see<br><br>where  is a positive monotone function (e.g.,  or softplus).<br>
pros:<br>
<br>stellar accuracy when you get the parameters right
<br>has good way of expressing dependence between diagonal variable  and off-diagonal variable <br>
problems:
<br>Nonlinear and nonconvex in the parameters 
<br>Requires significant effort to evaluate (each evaluation requires quadrature, which itself requires several evaluations of each multivariate polynomial, which requires several evaluations of each polynomial)
<br>Similar to above, a serious chore to invert
<br><br>Alternative to this, we have the separable linear parameterization, where<br><br>where  are monotone functions of some kind, and .<br>
pros:<br>
<br>Really fast. No quadrature
<br>Given s, you can find  in closed form<br>
cons:
<br>as described in the draft with Max, is that these do not capture cross-terms. I.e., any correlation between  and  cannot be described by this model. This almost surely creates bias in virtually all realistic situations (at least without particular care).
<br><br>What we need for a KR map is to ensure  for all . The idea of the integrated rectifier functions is easy to see from this part: just ensure the derivative is positive and then integrate that! This gives properties that are analytically very easy to evaluate.<br>However, what really happens is that the integrated rectifer "monotonizes" in the  direction (a cartoon would be to make something that looks like a bunch of sigmoids), and then just "positivizes" in the  directions. However, if we already have a monotone function in the  direction, then, we only need to "positivize" the  components. In this case, we get something of the following form:<br><br>with positive coefficients .<br><br>The major difference is that we're now adding terms dependent on both  in the diagonal derivative of the map. However, this only requires the addition of one evaluation of  (compared with the quadrature, which requires many new evaluations of the polynomials).<br><br>Consider a fixed . Then, we can sweep all of the -terms into constants and just consider that we approximate the function  using a log-sum. This is going to look really similar to basically approximating the diagonal-derivative in log-space using the cross-terms (indeed, if we instead chose to just separate the diagonal from rectified off-diagonal terms and then take the product there, we'd get that we are approximating the log determinant using polynomials). On the other hand, we still keep the linearity though.<br><br>I think so! It doesn't look like the formulation depends on the diagonal components not being dependent on , rather just that the offdiagonal components are not dependent on !<br>
Addendum: see <a data-href="Optimizing Linear Maps (new draft)" href="https://dsharp.dev/thesis/research/transport-intrinsics/optimizing-linear-maps-(new-draft).html" class="internal-link" target="_self" rel="noopener">Optimizing Linear Maps (new draft)</a><br><br>InputDerivative: gradient with respect to input
set f = 0, grad = [0,0,...,0]
for each midx and wrt each input (-1 for forward eval):
	if the midx == 0:
		if wrt == -1
			add coeff(0) to f
		end if
		continue
	end if
	
	initialize termVal=1, hasDeriv=false, wrtVal = 0, wrtDeriv = 0
	
	for each nonzero midx input dimension j up to J=nonzero_length(midx)-1:
		if dimension[j] == wrt (if this is the input we're differentiating wrt):
			wrtVal = value of psi[wrt] degree midx[wrt]
			wrtDeriv = deriv of psi[wrt] degree midx[wrt]
		else:
			termVal *= value of psi[j] degree midx[j]
		end if
	end for
	// Want to revert to loop body if not rectifying, only add code if we are
	d = last nonzero dimension of midx, i.e., dimension[nonzero_length(midx)]
	if d == wrt // If we differentiate wrt last nonconstant input
		wrtDeriv = deriv of psi[d] degree midx[d]
		if constexpr(rectify): // If there's rectification
			if d == dim: // Derivative wrt last input
				termVal = rectify(termVal) * wrtDeriv
			else: // no midx entry on last input
				wrtVal = value of psi[d] degree midx[d]
				termVal = deriv rectify(termVal*wrtVal)*termVal*wrtDeriv
		else constexpr: // Reduces to loop body
			termVal *= wrtDeriv
	else: // If we've "already" differentiated
		if constexpr(rectify):
			lastVal = value psi[d] degree midx[d]
			rectVal = d == dim ? termVal*wrtVal : termVal*lastVal*wrtVal
			if wrt == -1:
				diagVal = d == dim ? lastVal : 1
				termVal = rectify(rectVal)*diagVal
			else:
				// d_x f(p(x)q(y))g(z) = f'(p(x)q(y))p'(x)q(y)g(z)
				// we know deriv is not of g, so if d != dim, q takes place of g as lastVal
				termVal = rectify(rectVal)*termVal*wrtDeriv*lastVal
			end if
		else // Reduce to loop body
			lastVal = value of psi[d] degree midx[d] // already diff'd, don't need lastDeriv
			if w == -1:
				termVal = termVal*lastVal
			else:
				termVal = termVal*wrtDeriv*lastVal
			end if
		end if
	end if
	if wrt == -1
		f += termVal
	else
		grad(wrt) += termVal
	end if
end for
Copy]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/rectified-linear-parameterization.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Rectified Linear Parameterization.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Mon, 06 May 2024 20:46:58 GMT</pubDate></item><item><title><![CDATA[Sigmoid clustering]]></title><description><![CDATA[ 
 <br>Simple idea:<br>function SigmoidCluster1d(samples::Vector, k_max::Int)
	sort!(samples)
	ret = Vector{Vector{Float64}}(undef, k_max)
	for k in 1:k_max
		quant_idxs = get_k_quantile_idxs(samples, k)
		# Group the samples by index
		groups = [samples[idx_range] for idx_range in quant_idxs]
		# Get centers
		ret[k] = [mean(group) for group in groups]
	end
	return ret
end
Copy]]></description><link>https://dsharp.dev/thesis/research/transport-intrinsics/sigmoid-clustering.html</link><guid isPermaLink="false">Thesis/Research/Transport Intrinsics/Sigmoid clustering.md</guid><dc:creator><![CDATA[D Sharp]]></dc:creator><pubDate>Fri, 24 May 2024 18:42:55 GMT</pubDate></item></channel></rss>